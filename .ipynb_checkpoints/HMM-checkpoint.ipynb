{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Second-Order HMM for typos correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<font face=\"微软雅黑\"，size=4>  Taycir Yahmed (tyahmed@enst.fr), Mo Yang(lucasyeoh1992@gmail.com) and Zizhao LI (sharoncarl688@gmail.com)  \n",
    "\n",
    "January 2018</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Train a first-order HMM using the training data. This is basically what we did in lab sessions for POS-tagging. Compute the error rate (at the character level) and compare this results with the dummiest classifier that just do nothing. You can also compute the number of errors your model can correct and the number of errors your model creates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Second Order HMM: To improve the performance, we can increase the order of the HMM. Implement a second Order model for this task (this means that the probability of the next state depends on the current state and the previous one as well). A convenient way to implement a second order HMM, is to think about it as a variable change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Q1 and Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp; The first and second question is solved by HMM with Viterbi Algorithm. The task can be described as a supervised learning problem: when we see a word, for each obseverd letter, we want to predict which letter is most likely to be the correct letter. HMM is a statistical sequential model which is able to use baysian approach to make such prediction without dictionary. In the framework of HMM model, a state refers to the correct letter, a observation refers to the observed letter, given a sequence of observations, our task is to get the most possible sequence of states. Our project use Viterbi Algorithms for the task.   </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp; Excately like what we did in the previous tps, we first implement the first order of Viterbi algorithm. A HMM class is defined while integrated with both 1st order and 2sc order. For the first order HMM model, its parameters are defined below:  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> \n",
    " &ensp; &ensp;<b> First order HMM parameter </b>  \n",
    "  &ensp; &ensp;&ensp; &ensp;$ N :$ The number of states in a model $S^t = \\{s^t_1,s^t_2,...s^t_N\\}$ at the time $t$.  \n",
    "  &ensp; &ensp;&ensp; &ensp;$ M :$ The number of distinct observation symbols. $O = \\{o_1 , o_2 , · · · , o_M \\}$  \n",
    "  &ensp; &ensp;&ensp; &ensp;$ A=\\{a_{ij}\\} :  $ The set of N transition probability distributions. $a_{ij}=P(s^t_j | s^{t-1})_i$  \n",
    "  &ensp; &ensp;&ensp; &ensp;$ B=b_j(o_t):$ The observation probability distributions in state j.  $b_j(o_t)=P(o_t | s^t_j)$  \n",
    "  &ensp; &ensp;&ensp; &ensp;$ \\pi=\\{\\pi_i\\} $ The initial probability distribution\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp; Given the HMM model, we can give first order Viterbi Algorithms:  \n",
    "    <br>\n",
    " &ensp; &ensp;<b> Initialization: </b>  \n",
    " &ensp; &ensp; &ensp; &ensp;    [1] $\\delta_1(i)= \\pi_ib_i(o_1)$ &ensp;&ensp; $ 1 \\leq i \\leq N$  \n",
    " &ensp; &ensp; &ensp; &ensp;    [2] $j_1(i)=0$ &ensp;&ensp;$ 1 \\leq i \\leq N$  \n",
    " &ensp; &ensp;<b> Loop over </b>  \n",
    "  &ensp; &ensp;  &ensp; &ensp;  [3] $ \\delta_t(j)=max_{1 \\leq i \\leq N}[\\delta_{t-1}(i)a_{ij}]b_j(o_t)$ &ensp;&ensp; $2 \\leq t \\leq M ,1 \\leq j \\leq N $  \n",
    "   &ensp; &ensp;  &ensp; &ensp; [4] $ j_m(j)=argmax_{1 \\leq i \\leq N}[\\delta_{t-1}(i)a_{ij}]$ &ensp;&ensp; $2 \\leq t \\leq M ,1 \\leq j \\leq N  $  \n",
    " &ensp; &ensp;<b> End </b>  \n",
    "   &ensp; &ensp; &ensp; &ensp;  [5] $p^*=max_{1 \\leq i \\leq N}[\\delta_M(i)]$  \n",
    "   &ensp; &ensp; &ensp; &ensp;  [6] $q^*=argmax_{1 \\leq i \\leq N}[\\delta_M(i)]$\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp;The previous work on tps mainly focus on 1st order HMM model. As the most important property of 1st order HMM, the probability of the next state depend only on the current state but neither the previous state nor the state after. While, the context information is essential in our task. Thus 2sc or higher order HMM model is needed. Our works also contains implementation of 2sc order HMM Model. Here gives the model parameters:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> \n",
    " &ensp; &ensp;<b> Second order HMM parameter </b>  \n",
    " &ensp; &ensp; &ensp; &ensp;$ N :$ The number of states in a model $S^t = \\{s^t_1,s^t_2,...s^t_N\\}$ at the time $t$.  \n",
    " &ensp; &ensp; &ensp; &ensp;$ M :$ The number of distinct observation symbols. $O = \\{o_1 , o_2 , · · · , o_M \\}$  \n",
    " &ensp; &ensp; &ensp; &ensp;$ A=\\{a_{ijk}\\} :  $ The set of N transition probability distributions. $a_{ijk}=P(s^t_k | s^{t-2}_i, s^{t-1}_j)$  \n",
    " &ensp; &ensp; &ensp; &ensp;$ B=b_{k}(o_t):$ The observation probability distributions in state k.  $b_k(o_t)=P(o_k |s^t_k, s^{t-1}_j)$  \n",
    " &ensp; &ensp; &ensp; &ensp;$ \\pi=\\{\\pi_i\\} $ The initial probability distribution\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp;\n",
    "    One fundemental change from second order HMM model to first order HMM model is that the transition probability is based on two previous status, where $a_{ijk}=P(s^t_k | s^{t-2}_i, s^{t-1})_j$. Knowing that it is more difficult to find the sequence of 3 than the sequence of 2, the matrix A of second order will be sparser than which of first order. It is also counted later that only 2489 / 17576 sequence of 3 occur in our training data, while 403 / 676 sequence of 2 has shown. Matrix B has similar problem. So we introduce a two linear interpolation smoothing algorithm for Matrix A and Marix B. The approach is described as below:  \n",
    "    <br>\n",
    "    &ensp; &ensp;<b>For MatrixA:</b>  \n",
    "     &ensp; &ensp; &ensp; &ensp;$a_{ijk}=P(s^t_k | s^{t-2}_i, s^{t-1}_j)=\\lambda_1P(s^t_k | s^{t-2}_i, s^{t-1}_j) + \\lambda_2P(s^t_k | s^{t-1})_j + \\lambda_3P(s^t_k)$  \n",
    "     &ensp; &ensp; &ensp; &ensp;$\\lambda1 = k3$  \n",
    "     &ensp; &ensp; &ensp; &ensp;$\\lambda2 = (1-k3)*k2$  \n",
    "     &ensp; &ensp; &ensp; &ensp;$\\lambda3 = (1-k2)*(1-k2)$  \n",
    "     &ensp; &ensp; &ensp; &ensp;where &ensp; $k3 = \\frac{log(C(s^{t-2}_i, s^{t-1}_j, s^t_k)+1)+1}{log(C(s^{t-2}_i, s^{t-1}_j, s^t_k)+1)+2},$ &ensp; $k2 = \\frac{log(C(s^{t-1}_j, s^t_k)+1)+1}{log(C(s^{t-1}_j, s^t_k)+1)+2}$ &ensp; &ensp;  \n",
    "     &ensp; &ensp; &ensp; &ensp;$C(s^{t-2}_i, s^{t-1}_j, s^t_k)$ account for frequency of the sequence $s^{t-2}_i, s^{t-1}_j, s^t_k$ in training data  \n",
    "    <br>\n",
    "    &ensp; &ensp;<b>For MatrixB:</b>  \n",
    "     &ensp; &ensp; &ensp; &ensp;$b_k(o_t)=P(o_k |s^t_k, s^{t-1}_j)=(1- \\lambda) P(o_k |s^t_k, s^{t-1}_j) + \\lambda P(o_k |s^t_k)$  \n",
    "     &ensp; &ensp; &ensp; &ensp;where &ensp;$\\lambda = e^{-C(s^{t-1}_j, s^t_k, o_k)}$\n",
    "\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp;In our works we use two provided datasets to train and evaluate our model. They all comes from a publication \"Unabomber's Manifesto\" and are pre-processed. The dataset includes lowercase letters which are separated by only space, formatted and stored in a way like this: while each pair is constructed by the observered letter and the actual letter it should be.    \n",
    "    &ensp; &ensp;&ensp; &ensp;&ensp; &ensp;&ensp; &ensp;[('t', 't'), ('h', 'h'), ('w', 'e'), ('k', 'm')]  \n",
    "     &ensp; &ensp;&ensp; &ensp;&ensp; &ensp;&ensp; &ensp;[('f', 'f'), ('o', 'o'), ('r', 'r'), ('m', 'm')]  \n",
    "    &ensp; &ensp;One dataset has 10% error rate and the other has 20% error rate. We have tested on 1st and 2sc order hmm, also compared with a dummy classifier to compare how much the methode will boost. The result is as belows:  \n",
    "    &ensp; &ensp;\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><center>Dataset with error rate 10%</center></b>\n",
    "<table>\n",
    "   <tr>\n",
    "      <td></td>\n",
    "      <td>Nbs of errors exists</td>\n",
    "      <td>Nbs of errors the model correct</td>\n",
    "      <td>Nbs of errors the model create</td>\n",
    "      <td>Accurency</td>\n",
    "      <td>Precision</td>\n",
    "      <td>Recall</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>Dummy Classifier</td>\n",
    "      <td>745</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0.898</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>1st order HMM</td>\n",
    "      <td>745</td>\n",
    "      <td>322</td>\n",
    "      <td>328</td>\n",
    "      <td>0.912</td>\n",
    "      <td>0.495</td>\n",
    "      <td>0.505</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>2sec order HMM</td>\n",
    "      <td>745</td>\n",
    "      <td>418</td>\n",
    "      <td>186</td>\n",
    "      <td>0.940</td>\n",
    "      <td>0.692</td>\n",
    "      <td>0.626</td>\n",
    "   </tr>\n",
    "</table>\n",
    "<b><center>Dataset with error rate 20%</center></b>\n",
    "<table>\n",
    "   <tr>\n",
    "      <td></td>\n",
    "      <td>Nbs of errors exists</td>\n",
    "      <td>Nbs of errors the model correct</td>\n",
    "      <td>Nbs of errors the model create</td>\n",
    "      <td>Accurency</td>\n",
    "      <td>Precision</td>\n",
    "      <td>Recall</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>Dummy Classifier</td>\n",
    "      <td>3239</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0.805</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>1st order HMM</td>\n",
    "      <td>3239</td>\n",
    "      <td>1453</td>\n",
    "      <td>1423</td>\n",
    "      <td>0.843</td>\n",
    "      <td>0.505</td>\n",
    "      <td>0.548</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>2sec order HMM</td>\n",
    "      <td>3239</td>\n",
    "      <td>1966</td>\n",
    "      <td>848</td>\n",
    "      <td>0.895</td>\n",
    "      <td>0.698</td>\n",
    "      <td>0.686</td>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Critics and evolution: This model is somehow limited. For instance, it only handles substitution errors. Could you describe a way to extend this model to also handle noisy insertion of characters ?  \n",
    "### Q4 Same question for deletion (or omitted characters) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Q3 and Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp;\n",
    "    Hidden Markov model has a strict condition for the sequence of observation and state: the observation sequence and state sequence must have the same length, particularly each observation in the sequence must be related to one state in sequence and vice versa. The task for typos correction is different from POS-tagging. The model handles well substitution errors, but could not cope with noisy insertion and deletation errors. To correct such problem, A new approach is needed.  \n",
    "    <br>\n",
    "    Apparentely single letter should not be state or observation given the Markov property. Alternetively, the state space should be reconsidered. A method called 'Pindel' is useful to solve the problem.[] 'Pindel' is originally used for DNA mutation detection and is similar to our problem. The methode is introduced as below:  \n",
    "    <br>\n",
    "    &ensp; &ensp;\n",
    "    According to the markov property, a state generate a observation in a sequence. So we want to define states and observations that satisfy such property. In Pindel method, Assume that $S$ is the set of true sequence that generate read set $R$, in particular, $s_i \\in S$ will generate $i$-th read $(x_i, y_i)$, where $x_i$ and $y_i$ denote the basecalls and the quality scores. Let $s_i[j]$ denote the j-th subwindow of $s_i$ and $s_i[j...t]$ denote the substring of $s_i$ from position j to position t. Substings of length k or k+1 bases are named 'kmers' and '(k+1)mers'. Given the conception of 'kmers', for each read $x_i$: $x_{i,t} = x_i[t−k+1...t]$ will be the t-th observed kmer, which can be seen generated by $s_{i,t}$. $s_{i,t}$ will be either a 'kmers' or '(k+1)mers'. Here is how it works:  \n",
    "    <br>\n",
    "    &ensp; &ensp;\n",
    "    The sequence can be seen as sliding windows. We assume that the markov chain start at state $s_{i,k}$, which emit $(x_{i,k}, y_{i,k})$. Then, the sequencer transitions to the next state $s_{i,k+1}$, of course, emits $(x_{i,k+1}, y_{i,k+1})$. When the sequencer is at state $s_{i,t}$ for t > k, it emits $(x_{i[t]}, y_{i[t]})$, and sliding window begins. Although each $s_{i,t}$ correspond to a $(x_{i[t]}, y_{i[t]})$, the window position is probably not the same beacuse of insertion, deletion. Transitions between a 'kmer' and a '(k+1)mer' model deletion errors, transitions from a kmer to itself model insertion errors and the emission distribution models the substitution errors(like what we did in Q1 and Q2).  The following figure shows how sequence of states correspond to sequence of observation:  \n",
    "    ![](Pintel.png)  \n",
    "    <br>\n",
    "    &ensp; &ensp;Above figure is actually a DNA cut, the upper sequence is the observed DNA(seen stand for observation) and the bottom sequence is original DNA(seen stand for states). If we think of them as written letters and actual letters, it should be exactely the same problem. As shown the observations have all error types including insertion, deletion, sustitution. In the figure k=4. The figure show the process of sliding window. if the next basecall is insertion, the window stays. if the next observation is a deletion, the next state will be (k+1)mers. If it is a substitution error, the window will slide as normal but give a low score to $(x_{i[t]}, y_{i[t]})$. The three kinds of errors are modeled and the states sequence correspond exactely to observation sequences(basecalls).  \n",
    "    <br>\n",
    "    &ensp; &ensp; While a DNA sequence is also different from a word spelling. The lenth of DNA can be very large, the length of a word is often from 1 up to 10 or little bit more. In such case k don't need to be large. In our case we give these definitions to first order markov models:  \n",
    "    <br>\n",
    "    &ensp; &ensp;<b> State space </b>  \n",
    "    &ensp; &ensp; We denote $K$ to be the state space. Knowing that $K$ contains both kmers and (k+1)mers, we denote $K_1$ to all observed kmers and $K_2$ to all observed (k+1)mers. For each (k+1)mers $\\omega$ in $K_2$, $\\omega$ stand for a deletion position at end. To model insertion errors, we also denote $K_3 = \\{ \\omega^* : \\omega \\in K_1 \\}$ where $\\omega^*$ stand for all states that have been stayed. Thus, the state space $K$ is defined as follows:  \n",
    "    &ensp; &ensp; $K = K_1 \\bigcup K_2 \\bigcup K_3 $  \n",
    "    &ensp; &ensp;It is important to know that even $K_3 \\in K_1$, states they contains are actually different in our model. To differentiate them, we mark them differently and count differentely  \n",
    "    <br>\n",
    "    &ensp; &ensp;<b> Transition probability Matrix A</b>  \n",
    "    &ensp; &ensp;$A = a_{ij}=P(s_j^{t+1} | s_i^t)$  \n",
    "    &ensp; &ensp; as described above, we diffrenciate states comes from $K_1, K_2, K_3$. The matrix looks like this:\n",
    "    \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "   <tr>\n",
    "      <td></td>\n",
    "      <td>s1_1</td>\n",
    "      <td>s2_1</td>\n",
    "      <td>s3_1</td>\n",
    "      <td>s4_1</td>\n",
    "      <td>s5_1</td>\n",
    "      <td>s1_2</td>\n",
    "      <td>s2_2</td>\n",
    "      <td>s3_2</td>\n",
    "      <td>s1_3</td>\n",
    "      <td>s2_3</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>s1_1</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>s2_1</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>s3_1</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>s4_1</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>s5_1</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>s1_2</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>s2_2</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>s3_2</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>s1_3</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>s2_3</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp;\n",
    "    <b> Observations</b>  \n",
    "    &ensp; &ensp;&ensp; &ensp;$O = \\{ o_1, O_2, o_3,...o_m \\}$  \n",
    "    &ensp; &ensp; $o_t$ is defined as basecalls(letter).  \n",
    "    \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp;\n",
    "    <b> Emmition probability Matrix B</b>  \n",
    "    &ensp; &ensp;&ensp; &ensp;$B =\\{ b_j({o_t})\\}=P(o_t | s_j^t)$  \n",
    "    &ensp; &ensp; each $o_t$ will correspond to a $s_t$ in a sequence, $o_t$ and $s_t$ are defined above\n",
    "    \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp;\n",
    "    <b> Initial states distribution $\\pi=\\{\\pi_i$\\}</b>  \n",
    "    \n",
    "    \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp;\n",
    "    <b> Viterbi algorithms</b>  \n",
    "    &ensp; &ensp;&ensp; &ensp;The viterbi algorithms process is exactely the same as we did in Q1 and Q2. With VIterbi Algorithms we can get the most possible sequence of states.\n",
    "    \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp;\n",
    "    <b> Prediction</b>  \n",
    "    &ensp; &ensp;&ensp; &ensp;With the sequence of states we get from Viterbi, we can easily correct the three error types. We can process the sequence forward. If the state is in $K_3$, we will assume that there is a insertion error and we simply delete the letter. If the state is in $K_2$, we assume that there is a deletion error and we will add the last letter of the state which should be a (k+1)mers. If the last letter of the state does not match the observed letter, we assume that there is a subsituition error and we simply replace the letter.\n",
    "    \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp;\n",
    "    <b> Discussion</b>  \n",
    "    &ensp; &ensp;&ensp; &ensp; This method is only an idea so far, it is not implemented. If it is implemented, their could be many problems. One possible problem could be the choice of k. k can not be to large or too small. If k is too large, the total amount of states in K will be exploded and the transition matrix and emmition matrix will be too sparse. The lenth of the words is also a problem. If k is too small, the states in $K_1, K_2, K_3$ will lose much uniqueness thus statistically they will not be differenciated.    \n",
    "    &ensp; &ensp;&ensp; &ensp;The model can be extended to second or higher order\n",
    "    \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Unsupervised training: Propose and discuss some ideas to do unsupervised training for this task. For this question you should provide details on : what kind of data, which parameters will be involved, …"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\"，size=3> &ensp; &ensp; \n",
    "    &ensp; &ensp; \n",
    "    \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "import sys\n",
    "from numpy import unravel_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define HMM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummy_classifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def train(self, train):\n",
    "        pass\n",
    "    def predict(self, test):\n",
    "        result = []\n",
    "        for w in range(len(test)):\n",
    "            word_predicted = []\n",
    "            for i in range(len(test[w])):\n",
    "                word_predicted.append(test[w][i][0])\n",
    "            result.append(word_predicted)\n",
    "        return result\n",
    "    def score(self, test, result):\n",
    "        c_True_Positive = 0.0 \n",
    "        c_False_Positive = 0.0 \n",
    "        c_True_Negative = 0.0 \n",
    "        c_False_Negative = 0.0 \n",
    "            \n",
    "        if len(test) != len(result):\n",
    "            raise ValueError (\"Number of examples and results doesn't match\" )\n",
    "        n_examples = len(test)\n",
    "        for w in range(n_examples):\n",
    "            for i in range(len(test[w])):\n",
    "                if result[w][i] != test[w][i][0]:  #if the observation is considered not ture\n",
    "                    if result[w][i] == test[w][i][1]: #if the observation is correctely corrected\n",
    "                        c_True_Positive += 1\n",
    "                    else: #if the observation is uncorrectely corrected\n",
    "                        c_False_Positive += 1\n",
    "                else: #if the observation is considered ture\n",
    "                    if result[w][i] == test[w][i][1]: # if the observation is actually true\n",
    "                        c_True_Negative += 1\n",
    "                    else: #if the observation is actually false\n",
    "                        c_False_Negative += 1 \n",
    "            \n",
    "        print 'Number of errors exists:' + str(c_True_Positive + c_False_Negative)\n",
    "        print 'Number of errors the model correct' + str(c_True_Positive)\n",
    "        print 'Number of errors the model create' + str(c_False_Positive)\n",
    "        \n",
    "        print 'Accurency:' + str((c_True_Positive + c_True_Negative) / (c_True_Positive + c_False_Positive + c_True_Negative + c_False_Negative) )\n",
    "        print 'Precision:' + str(c_True_Positive / (c_True_Positive + c_False_Positive + epsilon) )\n",
    "        print 'Recall:' + str(c_True_Positive / (c_True_Positive + c_False_Negative+ epsilon) )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [
     391
    ]
   },
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, state_list, observation_list, order = 1,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None, smoothing_obs = 0.01):\n",
    "        \"\"\"\n",
    "            Builds a Hidden Markov Model\n",
    "            * state_list is the list of correct typed letters [q_0...q_(N-1)]\n",
    "            * observation_list is the list of observation letters [v_0...v_(M-1)]\n",
    "            \n",
    "            **first order HMM matrix:\n",
    "            * transition_proba is the transition probability matrix\n",
    "                [a_ij] a_ij = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            * observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            * initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\n",
    "            \n",
    "            **second order HMM matrix:\n",
    "            *transition_proba is the transition probability matrix which depend on the current state and the previous one as well\n",
    "                [a_ijk] a_ijk = Pr(Y_(t+1)=q_k|Y_t=q_j, Y_t-1=q_i)\n",
    "                \n",
    "                \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        print \"HMM creating with: \"\n",
    "        self.N = len(state_list)       # number of states\n",
    "        self.M = len(observation_list) # number of possible emissions\n",
    "        self.order = order\n",
    "        print str(self.N)+\" states\"\n",
    "        print str(self.M)+\" observations\"\n",
    "        print \"order = %d\" % self.order\n",
    "        self.omega_Y = state_list\n",
    "        self.omega_X = observation_list\n",
    "        self.end_state2ob_prob = None\n",
    "        self.single_state2ob_prob = None\n",
    "        \n",
    "        if self.order not in (1,2):\n",
    "            raise ValueError ('current HMM model only support first and second order') \n",
    " \n",
    "        if self.order == 1:\n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros( (self.N, self.N), float)\n",
    "            else:\n",
    "                self.transition_proba = transition_proba\n",
    "                \n",
    "            if observation_proba == None:\n",
    "                self.observation_proba = zeros( (self.M, self.N), float)\n",
    "            else:\n",
    "                self.observation_proba = observation_proba\n",
    "                \n",
    "        if self.order == 2:\n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros( (self.N, self.N, self.N), float)\n",
    "            else:\n",
    "                self.transition_proba = transition_proba\n",
    "            if observation_proba == None:\n",
    "                self.observation_proba = zeros( (self.N, self.N, self.M), float)\n",
    "            else:\n",
    "                self.observation_proba = observation_proba\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        if initial_state_proba is None:\n",
    "            self.initial_state_proba = zeros( (self.N,), float )\n",
    "        else:\n",
    "            self.initial_state_proba = initial_state_proba\n",
    "            \n",
    "        self.make_indexes() # build indexes, i.e the mapping between token and int\n",
    "        self.smoothing_obs = smoothing_obs\n",
    "        \n",
    "    def make_indexes(self):\n",
    "        \"\"\"Creates the reverse table that maps states/observations names\n",
    "        to their index in the probabilities array\"\"\"\n",
    "        self.Y_index = {}\n",
    "        for i in range(self.N):\n",
    "            self.Y_index[self.omega_Y[i]] = i\n",
    "            \n",
    "        self.X_index = {}\n",
    "        for i in range(self.M):\n",
    "            self.X_index[self.omega_X[i]] = i\n",
    "\n",
    "    def get_observationIndices( self, observations ):\n",
    "        \"\"\"return observation indices, i.e \n",
    "        return [self.O_index[o] for o in observations]\n",
    "        and deals with OOVs\n",
    "        \"\"\"\n",
    "        indices = zeros( len(observations), int )\n",
    "        k = 0\n",
    "        for o in observations:\n",
    "            if o in self.X_index:\n",
    "                indices[k] = self.X_index[o]\n",
    "            else:\n",
    "                indices[k] = UNKid\n",
    "            k += 1\n",
    "        return indices\n",
    "    \n",
    "    def data2indices(self, sent): \n",
    "        \"\"\"From one tagged sentence of the brown corpus: \n",
    "        - extract the words and tags \n",
    "        - returns two list of indices, one for each\n",
    "        -> (wordids, tagids)\n",
    "        \"\"\"\n",
    "        wordids = list()\n",
    "        tagids  = list()\n",
    "        for couple in sent:\n",
    "            wrd = couple[0]\n",
    "            tag = couple[1]\n",
    "            if wrd in self.X_index:\n",
    "                wordids.append(self.X_index[wrd])\n",
    "            else:\n",
    "                wordids.append(UNKid)\n",
    "            tagids.append(self.Y_index[tag])\n",
    "        return wordids,tagids\n",
    "    \n",
    "    def get_single_state2ob_prob(self, c_singles):\n",
    "        # fill with counts\n",
    "        single_state2ob_prob = zeros( (self.N, self.M), float)\n",
    "        for pair in c_singles:\n",
    "            alphabet_obv=pair[0] # get word \n",
    "            alphabet_cor=pair[1] # get tag  \n",
    "            cpt=c_singles[pair] # get the count\n",
    "            # get word index (row), deal with OOV\n",
    "            k = 0 # for <unk>\n",
    "            if alphabet_obv in self.X_index: \n",
    "                k=self.X_index[alphabet_obv]\n",
    "            # get tag  index (column)\n",
    "            i=self.Y_index[alphabet_cor]\n",
    "            # fill the matrix\n",
    "            single_state2ob_prob[i,k]=cpt\n",
    "        # normalize\n",
    "        single_state2ob_prob=single_state2ob_prob+self.smoothing_obs\n",
    "        single_state2ob_prob=single_state2ob_prob/single_state2ob_prob.sum(axis=0).reshape(1,self.N) \n",
    "        \n",
    "        self.single_state2ob_prob = single_state2ob_prob\n",
    "    \n",
    "    def get_end_state2ob_prob(self, c_ends):\n",
    "        # fill with counts\n",
    "        end_state2ob_prob = zeros( (self.N, self.M), float)\n",
    "        for pair in c_ends:\n",
    "            alphabet_obv=pair[0] # get word \n",
    "            alphabet_cor=pair[1] # get tag  \n",
    "            cpt=c_ends[pair] # get the count\n",
    "            # get word index (row), deal with OOV\n",
    "            k = 0 # for <unk>\n",
    "            if alphabet_obv in self.X_index: \n",
    "                k=self.X_index[alphabet_obv]\n",
    "            # get tag  index (column)\n",
    "            i=self.Y_index[alphabet_cor]\n",
    "            # fill the matrix\n",
    "            end_state2ob_prob[i,k]=cpt\n",
    "        # normalize\n",
    "        end_state2ob_prob=end_state2ob_prob+self.smoothing_obs\n",
    "        end_state2ob_prob=end_state2ob_prob/end_state2ob_prob.sum(axis=0).reshape(1,self.N) \n",
    "        \n",
    "        self.end_state2ob_prob = end_state2ob_prob\n",
    "    \n",
    "    def observation_estimation(self, pair_counts, c_obs_ij=None):\n",
    "        \"\"\" Build the observation distribution: \n",
    "            observation_proba is the observation probablility matrix\n",
    "                [b_ki],  b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "                \n",
    "            pair_counts is dictionary with \n",
    "            - key : a tuple (word,tag)\n",
    "            - value: the associated count\n",
    "                \n",
    "            We just need to fill the matrix and normalize the count in the right way: \n",
    "            - one column (i constant) is the distrib. of word given a tag\n",
    "            - just normalize the column, i.e sum over the row (axis=0)\n",
    "        \"\"\"\n",
    "        if self.order == 1:\n",
    "            self.observation_proba = self.observation_estimation_1(pair_counts)\n",
    "        if self.order == 2:\n",
    "            observation_proba_1 = self.observation_estimation_1(pair_counts)\n",
    "            observation_proba_2,observation_count_2 = self.observation_estimation_2(c_obs_ij)\n",
    "            it = np.nditer(self.observation_proba, flags=['multi_index'])\n",
    "            while not it.finished:\n",
    "                (i,j,k) = it.multi_index\n",
    "                lambda1 = np.exp(-observation_count_2[i,j,k])\n",
    "                self.observation_proba[i,j,k] = (1-lambda1)*observation_proba_2[i,j,k] + lambda1*observation_proba_1[j,k]\n",
    "                it.iternext()\n",
    "                \n",
    "        \n",
    "    def observation_estimation_1(self, pair_counts):\n",
    "        \"\"\" Build the observation distribution: \n",
    "            observation_proba is the observation probablility matrix\n",
    "                [b_ki],  b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "                \n",
    "            pair_counts is dictionary with \n",
    "            - key : a tuple (word,tag)\n",
    "            - value: the associated count\n",
    "                \n",
    "            We just need to fill the matrix and normalize the count in the right way: \n",
    "            - one column (i constant) is the distrib. of word given a tag\n",
    "            - just normalize the column, i.e sum over the row (axis=0)\n",
    "        \"\"\"\n",
    "        # fill with counts\n",
    "        observation_proba_1 = zeros( (self.N, self.M), float)\n",
    "        for pair in pair_counts:\n",
    "            alphabet_obv=pair[0] # get word \n",
    "            alphabet_cor=pair[1] # get tag  \n",
    "            cpt=pair_counts[pair] # get the count\n",
    "            # get word index (row), deal with OOV\n",
    "            k = 0 # for <unk>\n",
    "            if alphabet_obv in self.X_index: \n",
    "                k=self.X_index[alphabet_obv]\n",
    "            # get tag  index (column)\n",
    "            i=self.Y_index[alphabet_cor]\n",
    "            # fill the matrix\n",
    "            observation_proba_1[i,k]=cpt\n",
    "        # normalize\n",
    "        observation_proba_1=observation_proba_1+self.smoothing_obs\n",
    "        observation_proba_1=observation_proba_1/observation_proba_1.sum(axis=0).reshape(1,self.N) \n",
    "        self.observation_proba_1 = observation_proba_1\n",
    "        return observation_proba_1\n",
    "        \n",
    "    def observation_estimation_2(self, c_obs_ij):\n",
    "        observation_proba_2=zeros( (self.N, self.N, self.M), float)\n",
    "        observation_count_2=zeros( (self.N, self.N, self.M), float)\n",
    "        for obs_ij in c_obs_ij:\n",
    "            i = self.Y_index[obs_ij[0]]\n",
    "            j = self.Y_index[obs_ij[1]]\n",
    "            k = self.X_index[obs_ij[2]]\n",
    "            observation_proba_2[k,i,j] = c_obs_ij[obs_ij]\n",
    "            observation_count_2[i,j,k] = c_obs_ij[obs_ij]\n",
    "            \n",
    "        #normalize\n",
    "        observation_proba_2=observation_proba_2+self.smoothing_obs\n",
    "        observation_proba_2 = observation_proba_2 / observation_proba_2.sum(axis = 0)\n",
    "        observation_proba_2 = np.rollaxis(observation_proba_2, 2)\n",
    "        observation_proba_2 = np.rollaxis(observation_proba_2, 2)\n",
    "        return observation_proba_2, observation_count_2\n",
    "        \n",
    "    def transition_estimation_1(self, state_counts):\n",
    "        self_transition_proba = zeros( (self.N), float)\n",
    "        for state in state_counts:\n",
    "            if len(state) != 1:\n",
    "                raise ValueError ('self_transition must have only counts for single state')\n",
    "            state_idx = self.Y_index[state]\n",
    "            self_transition_proba[state_idx] = state_counts[state]\n",
    "        #normalize\n",
    "        self_transition_proba = self_transition_proba+self.smoothing_obs\n",
    "        self_transition_proba = self_transition_proba / sum(self_transition_proba)\n",
    "        return self_transition_proba\n",
    "    \n",
    "    def transition_estimation_2(self, trans_counts):\n",
    "        \"\"\" Build the transition distribution: \n",
    "            transition_proba is the transition matrix with : \n",
    "            [a_ij] a[i,j] = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "                \n",
    "            trans_counts is dictionary with \n",
    "            - key : a tuple (tag-1,tag)\n",
    "            - value: the associated count\n",
    "                \n",
    "             We just need to fill the matrix and normalize the count in the right way: \n",
    "            - one column (i constant) is the distrib. of tags for  given context tag-1\n",
    "            - just normalize the column, i.e sum over the row (axis=0)\n",
    "        \"\"\"\n",
    "        \n",
    "        # fill with counts\n",
    "        transition_proba_2 = zeros( (self.N, self.N), float)\n",
    "        transition_count_2 = zeros( (self.N, self.N), float)\n",
    "        for pair in trans_counts:\n",
    "            if len(pair) != 2:\n",
    "                raise ValueError ('first order state transition must have only two states')\n",
    "            i=self.Y_index[pair[0]]\n",
    "            j=self.Y_index[pair[1]]\n",
    "            transition_count_2[i,j]=trans_counts[pair]\n",
    "            transition_proba_2[j,i]=trans_counts[pair]\n",
    "        # normalize\n",
    "        transition_proba_2=transition_proba_2/transition_proba_2.sum(axis=0)\n",
    "        transition_proba_2=np.rollaxis(transition_proba_2,1)\n",
    "        self.transition_proba_2 = transition_proba_2\n",
    "        return transition_proba_2, transition_count_2\n",
    "        \n",
    "    def transition_estimation_3(self, tri_trans_counts):\n",
    "        transition_proba_3 = zeros( (self.N, self.N, self.N), float)\n",
    "        transition_count_3 = zeros( (self.N, self.N, self.N), float)\n",
    "        for tri_states in tri_trans_counts:\n",
    "            i = self.Y_index[tri_states[0]]\n",
    "            j = self.Y_index[tri_states[1]]\n",
    "            k = self.Y_index[tri_states[2]]\n",
    "            transition_count_3[i,j,k]=tri_trans_counts[tri_states]\n",
    "            transition_proba_3[k,i,j]=tri_trans_counts[tri_states]\n",
    "        #normalize\n",
    "        transition_proba_3=transition_proba_3+self.smoothing_obs  \n",
    "        transition_proba_3=transition_proba_3/transition_proba_3.sum(axis=0)\n",
    "        transition_proba_3=np.rollaxis(transition_proba_3, 2)\n",
    "        transition_proba_3=np.rollaxis(transition_proba_3, 2)\n",
    "        return transition_proba_3, transition_count_3\n",
    "        \n",
    "    def transition_estimation(self, trans_counts, state_counts=None, tri_trans_counts=None):\n",
    "        if self.order == 1:\n",
    "            self.transition_proba, transition_count_2 = self.transition_estimation_2(trans_counts)\n",
    "        #combine 0 order, 1st order, 2sc order transition estimation\n",
    "        else:\n",
    "            self.transition_proba = zeros( (self.N, self.N, self.N), float)\n",
    "            transition_proba_1 = self.transition_estimation_1(state_counts)\n",
    "            transition_proba_2, transition_count_2 = self.transition_estimation_2(trans_counts)\n",
    "            transition_proba_3, transition_count_3 = self.transition_estimation_3(tri_trans_counts)\n",
    "            it = np.nditer(self.transition_proba, flags=['multi_index'])\n",
    "            while not it.finished:\n",
    "                (i,j,k) = it.multi_index\n",
    "                l3 = (np.log(transition_count_3[i,j,k]+1)+1)/(np.log(transition_count_3[i,j,k]+1)+2)\n",
    "                l2 = (np.log(transition_count_2[j,k]+1)+1)/(np.log(transition_count_2[j,k]+1)+2)\n",
    "                labmda_1 = l3\n",
    "                labmda_2 = (1-l3)*l2\n",
    "                labmda_3 = (1-l3)*(1-l2)\n",
    "                self.transition_proba[i,j,k] = labmda_1 * transition_proba_3[i,j,k] + labmda_2 * transition_proba_2[j,k] + labmda_3 * transition_proba_1[k]\n",
    "                it.iternext()\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    def init_estimation(self, init_counts):\n",
    "        \"\"\"Build the init. distribution\"\"\"\n",
    "        # fill with counts\n",
    "        for tag in init_counts:\n",
    "            i=self.Y_index[tag]\n",
    "            self.initial_state_proba[i]=init_counts[tag]\n",
    "        # normalize\n",
    "        self.initial_state_proba=self.initial_state_proba/sum(self.initial_state_proba)\n",
    "             \n",
    "        \n",
    "    def supervised_training(self,init_counts, pair_counts, c_transitions_2, c_singles, state_counts=None, c_transitions_3=None, c_obs_ij=None, c_ends = None):\n",
    "        \"\"\" Train the HMM's parameters. This function wraps everything\"\"\"\n",
    "        \n",
    "        self.init_estimation(init_counts)\n",
    "        self.get_single_state2ob_prob(c_singles)\n",
    "        \n",
    "        \n",
    "    \n",
    "        if self.order == 1:\n",
    "            if c_transitions_2 is None:\n",
    "                raise ValueError ('to train a first order HMM, c_transitions_2 must be provided')\n",
    "            self.transition_estimation(c_transitions_2)\n",
    "            if pair_counts is None:\n",
    "                raise ValueError ('to train a first order HMM, pair_counts must be provided')\n",
    "            self.observation_estimation(pair_counts)\n",
    "            \n",
    "            \n",
    "            \n",
    "        elif self.order == 2:\n",
    "            if state_counts is None:\n",
    "                raise ValueError ('to train a second order HMM, state_counts must be provided')   \n",
    "            if c_transitions_2 is None:\n",
    "                raise ValueError ('to train a second order HMM, c_transitions_2 must be provided')\n",
    "            if c_transitions_3 is None:\n",
    "                raise ValueError ('to train a second order HMM, c_transitions_3 must be provided')\n",
    "            self.transition_estimation(c_transitions_2, state_counts, c_transitions_3)\n",
    "            \n",
    "            if pair_counts is None:\n",
    "                raise ValueError ('to train a second order HMM, pair_counts must be provided')\n",
    "            if c_obs_ij is None:\n",
    "                raise ValueError ('to train a second order HMM, c_obs_ij must be provided')\n",
    "            self.observation_estimation(pair_counts, c_obs_ij)\n",
    "            \n",
    "            self.get_end_state2ob_prob(c_ends)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            raise ValueError ('hmm order must be either 1 or 2')\n",
    "        \n",
    "    \n",
    "    def predict(self, test):\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        if self.order == 1:\n",
    "            for word in test:\n",
    "                predicted_word = []\n",
    "                if len(word) == 1:\n",
    "                    ob = obs[0]\n",
    "                    predicted_word_idx = np.argmax(self.single_state2ob_prob[:,ob])\n",
    "                    predicted_word.append(self.omega_Y[predicted_word_idx])\n",
    "                else:\n",
    "                    obs, cors = self.data2indices(word)\n",
    "                    c_letters = len(word)\n",
    "                    ob_state_prob = np.zeros((c_letters, self.N))\n",
    "                    routes_trans = np.zeros((c_letters, self.N))\n",
    "                    best_route_idx = np.zeros(c_letters)\n",
    "                    for t, ob in enumerate(obs):\n",
    "                        if t ==0:\n",
    "                            ob_state_prob[t] = self.initial_state_proba * self.observation_proba[:,ob]\n",
    "                        else:\n",
    "                            for i in range(self.N):\n",
    "                                ob_state_prob[t][i] = np.max(ob_state_prob[t-1] * self.transition_proba[:,i]) * self.observation_proba[i,ob]   \n",
    "                                routes_trans[t][i] = np.argmax(ob_state_prob[t-1] * self.transition_proba[:,i])\n",
    "\n",
    "                    best_route_idx[-1] = np.argmax(ob_state_prob[-1])\n",
    "                    for a in range(2,c_letters+1):\n",
    "                        best_route_idx[-a] = routes_trans[-a+1][int(best_route_idx[-a+1])]\n",
    "                    for idx in best_route_idx:\n",
    "                        predicted_word.append(self.omega_Y[int(idx)])\n",
    "                \n",
    "                result.append(predicted_word)\n",
    "            return  result \n",
    "        \n",
    "        if self.order == 2:\n",
    "            for word in test:\n",
    "                obs, cors = self.data2indices(word)\n",
    "                predicted_word = []\n",
    "                if len(word) == 1:\n",
    "                    ob = obs[0]\n",
    "                    predicted_word_idx = np.argmax(self.single_state2ob_prob[:,ob])\n",
    "                    predicted_word.append(self.omega_Y[predicted_word_idx])\n",
    "                else:\n",
    "                    c_letters = len(word)\n",
    "                    ob_state_prob = np.zeros((c_letters, self.N, self.N))\n",
    "                    routes_trans = np.zeros((c_letters, self.N, self.N))\n",
    "                    best_route_idx = np.zeros(c_letters)\n",
    "                    for t, ob in enumerate(obs):\n",
    "                        if t ==0:\n",
    "                            delta_i = self.initial_state_proba * self.observation_proba_1[:,ob].reshape(1,self.N)\n",
    "                            #print delta_i\n",
    "                        if t ==1:\n",
    "                            it = np.nditer(ob_state_prob[t], flags=['multi_index'])\n",
    "                            while not it.finished:\n",
    "                                (i,j) = it.multi_index\n",
    "                                ob_state_prob[t,i,j] = delta_i[0,i] * self.transition_proba_2[i,j] * self.observation_proba_1[j,ob]\n",
    "                                routes_trans[t,i,j] = 0\n",
    "                                #print ob_state_prob[1]\n",
    "                                it.iternext()\n",
    "                        else:\n",
    "                            it = np.nditer(ob_state_prob[t], flags=['multi_index'])\n",
    "                            while not it.finished:\n",
    "                                (j,k) = it.multi_index\n",
    "                                ob_state_prob[t,j,k] = np.max(ob_state_prob[t-1,:,j] * self.transition_proba[:,j,k]) * self.observation_proba_1[k,ob]\n",
    "                                routes_trans[t,j,k] = np.argmax(ob_state_prob[t-1,:,j] * self.transition_proba[:,j,k])\n",
    "                                it.iternext()\n",
    "                            \n",
    "                    (j,k) = unravel_index(ob_state_prob[-1].argmax(), ob_state_prob[-1].shape)\n",
    "                    best_route_idx[-2],best_route_idx[-1] = j, k\n",
    "                    for t in range(c_letters-3,-1,-1):\n",
    "                        best_route_idx[t] = routes_trans[t+2][int(best_route_idx[t+1]),int(best_route_idx[t+2])]\n",
    "                    for idx in best_route_idx:\n",
    "                        predicted_word.append(self.omega_Y[int(idx)])\n",
    "                result.append(predicted_word)\n",
    "            return  result\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        if self.order == 2:\n",
    "        \n",
    "            \n",
    "            for word in test:\n",
    "                predicted_word = []\n",
    "                obs, cors = self.data2indices(word)\n",
    "                \n",
    "                if len(word) == 1:\n",
    "                    ob = obs[0]\n",
    "                    predicted_word_idx = np.argmax(self.single_state2ob_prob[:,ob])\n",
    "                    predicted_word.append(self.omega_Y[predicted_word_idx])\n",
    "                else:\n",
    "                    c_letters = len(word)\n",
    "                    ob_state_prob = np.zeros((c_letters, self.N, self.N))\n",
    "                    routes_trans = np.zeros((c_letters, self.N, self.N))\n",
    "                    best_route_idx = np.zeros(c_letters)\n",
    "                    for t, ob in enumerate(obs):\n",
    "                        if t ==0:\n",
    "                            delta = self.initial_state_proba * self.observation_proba_1[:,ob]\n",
    "                            it = np.nditer(ob_state_prob[t], flags=['multi_index'])\n",
    "                            while not it.finished:\n",
    "                                (i,j) = it.multi_index\n",
    "                                ob_state_prob[t,i,j] = self.initial_state_proba[i] * self.observation_proba[i,j,ob]\n",
    "                                routes_trans[t,i,j] = 0\n",
    "                                it.iternext()\n",
    "                    \n",
    "                        else:\n",
    "                            it = np.nditer(ob_state_prob[t], flags=['multi_index'])\n",
    "                            while not it.finished:\n",
    "                                (j,k) = it.multi_index\n",
    "                                ob_state_prob[t,j,k] = np.max(ob_state_prob[t-1,:,j] * self.transition_proba[:,j,k]) * self.observation_proba[j,k,ob]\n",
    "                                routes_trans[t,j,k] = np.argmax(ob_state_prob[t-1,:,j] * self.transition_proba[:,j,k])\n",
    "                                if t == (c_letters - 1):\n",
    "                                    ob_state_prob[t,j,k] = ob_state_prob[t,j,k] * self.end_state2ob_prob[k,ob]\n",
    "                                it.iternext()\n",
    "                            \n",
    "                    (j,k) = unravel_index((ob_state_prob[-1] ).argmax(), ob_state_prob[-1].shape)\n",
    "                    best_route_idx[-2],best_route_idx[-1] = j, k\n",
    "                    for a in range(3,c_letters+1):\n",
    "                        best_route_idx[-a] = routes_trans[-a+2][int(best_route_idx[-a+1]),int(best_route_idx[-a+2])]\n",
    "                    for idx in best_route_idx:\n",
    "                        predicted_word.append(self.omega_Y[int(idx)])\n",
    "                result.append(predicted_word)\n",
    "                \n",
    "            return  result\n",
    "    '''    \n",
    "        \n",
    "    def score(self, test, result):\n",
    "        c_True_Positive = 0.0\n",
    "        c_False_Positive = 0.0\n",
    "        c_True_Negative = 0.0\n",
    "        c_False_Negative = 0.0\n",
    "        c_errors  = 0.0\n",
    "            \n",
    "        if len(test) != len(result):\n",
    "            raise ValueError (\"Number of examples and results doesn't match\" )\n",
    "        n_examples = len(test)\n",
    "        for w in range(n_examples):\n",
    "            for i in range(len(test[w])):\n",
    "                \n",
    "                if test[w][i][1] != test[w][i][0]:\n",
    "                    c_errors += 1\n",
    "                    \n",
    "                if result[w][i] != test[w][i][0]:  #if the observation is considered not ture\n",
    "                    if result[w][i] == test[w][i][1]: #if the observation is correctely corrected\n",
    "                        c_True_Positive += 1\n",
    "                    else: #if the observation is uncorrectely corrected\n",
    "                        c_False_Positive += 1\n",
    "                else: #if the observation is considered ture\n",
    "                    if result[w][i] == test[w][i][1]: # if the observation is actually true\n",
    "                        c_True_Negative += 1\n",
    "                    else: #if the observation is actually false\n",
    "                        c_False_Negative += 1 \n",
    "            \n",
    "        print 'Number of errors exists:' + str(c_errors)\n",
    "        print 'Number of errors the model correct' + str(c_True_Positive)\n",
    "        print 'Number of errors the model create' + str(c_False_Positive)\n",
    "        \n",
    "        print 'Accurency:' + str((c_True_Positive + c_True_Negative) / (c_True_Positive + c_False_Positive + c_True_Negative + c_False_Negative) )\n",
    "        print 'Precision:' + str(c_True_Positive / (c_True_Positive + c_False_Positive) )\n",
    "        print 'Recall:' + str(c_True_Positive / (c_True_Positive + c_False_Negative) )\n",
    "                        \n",
    "                        \n",
    "           \n",
    "            \n",
    "                            \n",
    "          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_counts(corpus):\n",
    "    \"\"\" \n",
    "    Build different count tables to train a HMM. Each count table is a dictionnary. \n",
    "    Returns: \n",
    "    * c_words: word counts\n",
    "    * c_tags: tag counts\n",
    "    * c_pairs: count of pairs (word,tag)\n",
    "    * c_transitions: count of tag bigram \n",
    "    * c_inits: count of tag found in the first position\n",
    "    \"\"\"\n",
    "    c_states = dict()\n",
    "    c_observations = dict()\n",
    "    c_pairs= dict()\n",
    "    c_transitions_2 = dict()\n",
    "    c_transitions_3 = dict()\n",
    "    c_inits = dict()\n",
    "    c_obs_ij = dict()\n",
    "    c_ends = dict()\n",
    "    c_singles = dict()\n",
    "    for word in corpus:\n",
    "        # we use i because of the transition counts\n",
    "        for i in range(len(word)):\n",
    "            couple=word[i]\n",
    "            alphabet_obv = couple[0]\n",
    "            alphabet_cor = couple[1]\n",
    "            # obv counts\n",
    "            if alphabet_obv in c_observations:\n",
    "                c_observations[alphabet_obv] += 1\n",
    "            else:\n",
    "                c_observations[alphabet_obv] = 1\n",
    "            # state counts\n",
    "            if alphabet_cor in c_states:\n",
    "                c_states[alphabet_cor] += 1\n",
    "            else:\n",
    "                c_states[alphabet_cor] = 1\n",
    "            # state-obvs counts\n",
    "            if couple in c_pairs:\n",
    "                c_pairs[couple] += 1\n",
    "            else:\n",
    "                c_pairs[couple]=1\n",
    "            # i >  0 -> transition counts\n",
    "            if i > 0:\n",
    "                trans_2 = (word[i-1][1],alphabet_cor) # (tag at t-1 , tag at t)\n",
    "                obs_ij = (word[i-1][1],alphabet_cor,word[i][0]) # state at t-1, state at t, obs at t-1\n",
    "                if trans_2 in c_transitions_2:\n",
    "                    c_transitions_2[trans_2] += 1\n",
    "                else:\n",
    "                    c_transitions_2[trans_2]=1\n",
    "                if obs_ij in c_obs_ij:\n",
    "                    c_obs_ij[obs_ij] += 1\n",
    "                else:\n",
    "                    c_obs_ij[obs_ij] = 1\n",
    "                \n",
    "            if i > 1:\n",
    "                trans_3 = (word[i-2][1], word[i-1][1], alphabet_cor)# (state at t-2 , state at t-1, state at t)\n",
    "                if trans_3 in c_transitions_3:\n",
    "                    c_transitions_3[trans_3] += 1\n",
    "                else:\n",
    "                    c_transitions_3[trans_3]=1\n",
    "            # i == 0 -> counts for initial states\n",
    "            else:\n",
    "                if alphabet_cor in c_inits:\n",
    "                    c_inits[alphabet_cor] += 1\n",
    "                else:\n",
    "                    c_inits[alphabet_cor] = 1\n",
    "        \n",
    "        last_couple=word[-1]\n",
    "        if last_couple in c_ends:\n",
    "            c_ends[last_couple] += 1\n",
    "        else:\n",
    "            c_ends[last_couple] = 1\n",
    "            \n",
    "        if len(word) == 1:\n",
    "            couple=word[0]\n",
    "            if couple in c_singles:\n",
    "                c_singles[couple] += 1\n",
    "            else:\n",
    "                c_singles[couple] = 1\n",
    "                    \n",
    "    return c_observations,c_states,c_pairs, c_transitions_2, c_transitions_3, c_inits, c_obs_ij, c_ends, c_singles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('typos-data/train10.pkl','rb')\n",
    "train10 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('typos-data/test10.pkl','rb')\n",
    "test10 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('typos-data/train20.pkl','rb')\n",
    "train20 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('typos-data/test20.pkl','rb')\n",
    "test20 = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1:\n",
      "Number of words for train = 29057\n",
      "Number of words for test  = 1501\n",
      "dataset2:\n",
      "Number of words for train = 27184\n",
      "Number of words for test  = 3374\n"
     ]
    }
   ],
   "source": [
    "print \"dataset1:\"\n",
    "print \"Number of words for train = \"+str(len(train10))\n",
    "print \"Number of words for test  = \"+str(len(test10))\n",
    "print \"dataset2:\"\n",
    "print \"Number of words for train = \"+str(len(train20))\n",
    "print \"Number of words for test  = \"+str(len(test20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of dummiest classifier with dataset1(train10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dummy_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.train(train10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dc.predict(test10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors exists:745.0\n",
      "Number of errors the model correct0.0\n",
      "Number of errors the model create0.0\n",
      "Accurency:0.898224043716\n",
      "Precision:0.0\n",
      "Recall:0.0\n"
     ]
    }
   ],
   "source": [
    "dc.score(test10, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of dummiest classifier with dataset2(train20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors exists:3239.0\n",
      "Number of errors the model correct0.0\n",
      "Number of errors the model create0.0\n",
      "Accurency:0.805943322749\n",
      "Precision:0.0\n",
      "Recall:0.0\n"
     ]
    }
   ],
   "source": [
    "dc = dummy_classifier()\n",
    "dc.train(train20)\n",
    "result = dc.predict(test20)\n",
    "dc.score(test20, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get statistical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for dataset1(train10)\n",
      "Number of states(correct typed letters)  : 26\n",
      "Nomber of observations(actually typed letters)  : 26\n",
      "Number of pairs(state-obervations): 127\n",
      "Number of first order trans(state(t-1)-state(t)) : 403 / 676\n",
      "Number of second order trans(state(t-1)-state(t)) : 2489 / 17576\n",
      "Nombre de init. : 26\n",
      "Number of obs_ij:1490\n",
      "Number of single letter word:27\n"
     ]
    }
   ],
   "source": [
    "c_observations,c_states,c_pairs, c_transitions_2, c_transitions_3, c_inits, c_obs_ij, c_ends, c_singles = make_counts(train10)\n",
    "print \"parameters for dataset1(train10)\"\n",
    "print \"Number of states(correct typed letters)  : \"+str(len(c_states))\n",
    "print \"Nomber of observations(actually typed letters)  : \"+str(len(c_observations))\n",
    "print \"Number of pairs(state-obervations): \"+str(len(c_pairs))\n",
    "print \"Number of first order trans(state(t-1)-state(t)) : \"+str(len(c_transitions_2))+ \" / \"+ str(26*26)\n",
    "print \"Number of second order trans(state(t-1)-state(t)) : \" + str(len(c_transitions_3))+ \" / \"+ str(26*26*26)\n",
    "print \"Nombre de init. : \"+str(len(c_inits))\n",
    "print \"Number of obs_ij:\" +str(len(c_obs_ij))\n",
    "print \"Number of single letter word:\" +str(len(c_singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create first order HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "26 states\n",
      "26 observations\n",
      "order = 1\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(state_list=c_states.keys(), observation_list=c_observations.keys(), order = 1,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None,\n",
    "                 smoothing_obs = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "hmm.observation_estimation(c_pairs)\n",
    "print hmm.observation_proba.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "hmm.transition_estimation_2(c_transitions_2)\n",
    "print hmm.transition_proba_2.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "hmm.init_estimation(c_inits)\n",
    "print sum(hmm.initial_state_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train first order HMM with dataset1 (train 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for dataset1(train10)\n",
      "Number of states(correct typed letters)  : 26\n",
      "Nomber of observations(actually typed letters)  : 26\n",
      "Number of pairs(state-obervations): 127\n",
      "Number of first order trans(state(t-1)-state(t)) : 403 / 676\n",
      "Number of second order trans(state(t-1)-state(t)) : 2489 / 17576\n",
      "Nombre de init. : 26\n",
      "Number of obs_ij:1490\n",
      "Number of single letter word:27\n"
     ]
    }
   ],
   "source": [
    "c_observations,c_states,c_pairs, c_transitions_2, c_transitions_3, c_inits, c_obs_ij, c_ends, c_singles = make_counts(train10)\n",
    "print \"parameters for dataset1(train10)\"\n",
    "print \"Number of states(correct typed letters)  : \"+str(len(c_states))\n",
    "print \"Nomber of observations(actually typed letters)  : \"+str(len(c_observations))\n",
    "print \"Number of pairs(state-obervations): \"+str(len(c_pairs))\n",
    "print \"Number of first order trans(state(t-1)-state(t)) : \"+str(len(c_transitions_2))+ \" / \"+ str(26*26)\n",
    "print \"Number of second order trans(state(t-1)-state(t)) : \" + str(len(c_transitions_3))+ \" / \"+ str(26*26*26)\n",
    "print \"Nombre de init. : \"+str(len(c_inits))\n",
    "print \"Number of obs_ij:\" +str(len(c_obs_ij))\n",
    "print \"Number of single letter word:\" +str(len(c_singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "hmm.supervised_training(c_inits,c_pairs,c_transitions_2, c_singles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hmm.predict(test10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors exists:745.0\n",
      "Number of errors the model correct322.0\n",
      "Number of errors the model create328.0\n",
      "Accurency:0.912158469945\n",
      "Precision:0.495384615385\n",
      "Recall:0.505494505495\n"
     ]
    }
   ],
   "source": [
    "hmm.score(test10,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train first order HMM with dataset2 (train20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for dataset1(train20)\n",
      "Number of states(correct typed letters)  : 26\n",
      "Nomber of observations(actually typed letters)  : 26\n",
      "Number of pairs(state-obervations): 128\n",
      "Number of first order trans(state(t-1)-state(t)) : 401 / 676\n",
      "Number of second order trans(state(t-1)-state(t)) : 2464 / 17576\n",
      "Nombre de init. : 26\n",
      "Number of obs_ij:1572\n",
      "Number of single letter word:29\n"
     ]
    }
   ],
   "source": [
    "c_observations,c_states,c_pairs, c_transitions_2, c_transitions_3, c_inits, c_obs_ij, c_ends, c_singles = make_counts(train20)\n",
    "print \"parameters for dataset1(train20)\"\n",
    "print \"Number of states(correct typed letters)  : \"+str(len(c_states))\n",
    "print \"Nomber of observations(actually typed letters)  : \"+str(len(c_observations))\n",
    "print \"Number of pairs(state-obervations): \"+str(len(c_pairs))\n",
    "print \"Number of first order trans(state(t-1)-state(t)) : \"+str(len(c_transitions_2))+ \" / \"+ str(26*26)\n",
    "print \"Number of second order trans(state(t-1)-state(t)) : \" + str(len(c_transitions_3))+ \" / \"+ str(26*26*26)\n",
    "print \"Nombre de init. : \"+str(len(c_inits))\n",
    "print \"Number of obs_ij:\" +str(len(c_obs_ij))\n",
    "print \"Number of single letter word:\" +str(len(c_singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.supervised_training(c_inits,c_pairs,c_transitions_2, c_singles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hmm.predict(test20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors exists:3239.0\n",
      "Number of errors the model correct1453.0\n",
      "Number of errors the model create1423.0\n",
      "Accurency:0.843089089929\n",
      "Precision:0.505215577191\n",
      "Recall:0.548508871272\n"
     ]
    }
   ],
   "source": [
    "hmm.score(test20,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create second order HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "26 states\n",
      "26 observations\n",
      "order = 2\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(state_list=c_states.keys(), observation_list=c_observations.keys(), order = 2,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None,\n",
    "                 smoothing_obs = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train second order HMM with dataset1 (train10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for dataset1(train10)\n",
      "Number of states(correct typed letters)  : 26\n",
      "Nomber of observations(actually typed letters)  : 26\n",
      "Number of pairs(state-obervations): 127\n",
      "Number of first order trans(state(t-1)-state(t)) : 403 / 676\n",
      "Number of second order trans(state(t-1)-state(t)) : 2489 / 17576\n",
      "Nombre de init. : 26\n",
      "Number of obs_ij:1490\n",
      "Number of single letter word:27\n"
     ]
    }
   ],
   "source": [
    "c_observations,c_states,c_pairs, c_transitions_2, c_transitions_3, c_inits, c_obs_ij, c_ends, c_singles = make_counts(train10)\n",
    "print \"parameters for dataset1(train10)\"\n",
    "print \"Number of states(correct typed letters)  : \"+str(len(c_states))\n",
    "print \"Nomber of observations(actually typed letters)  : \"+str(len(c_observations))\n",
    "print \"Number of pairs(state-obervations): \"+str(len(c_pairs))\n",
    "print \"Number of first order trans(state(t-1)-state(t)) : \"+str(len(c_transitions_2))+ \" / \"+ str(26*26)\n",
    "print \"Number of second order trans(state(t-1)-state(t)) : \" + str(len(c_transitions_3))+ \" / \"+ str(26*26*26)\n",
    "print \"Nombre de init. : \"+str(len(c_inits))\n",
    "print \"Number of obs_ij:\" +str(len(c_obs_ij))\n",
    "print \"Number of single letter word:\" +str(len(c_singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.supervised_training(c_inits, c_pairs, c_transitions_2, c_singles, c_states, c_transitions_3, c_obs_ij, c_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hmm.predict(test10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors exists:745.0\n",
      "Number of errors the model correct418.0\n",
      "Number of errors the model create186.0\n",
      "Accurency:0.94043715847\n",
      "Precision:0.692052980132\n",
      "Recall:0.625748502994\n"
     ]
    }
   ],
   "source": [
    "hmm.score(test10, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train second order HMM with dataset2 (train20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters for dataset1(train20)\n",
      "Number of states(correct typed letters)  : 26\n",
      "Nomber of observations(actually typed letters)  : 26\n",
      "Number of pairs(state-obervations): 128\n",
      "Number of first order trans(state(t-1)-state(t)) : 401 / 676\n",
      "Number of second order trans(state(t-1)-state(t)) : 2464 / 17576\n",
      "Nombre de init. : 26\n",
      "Number of obs_ij:1572\n",
      "Number of single letter word:29\n"
     ]
    }
   ],
   "source": [
    "c_observations,c_states,c_pairs, c_transitions_2, c_transitions_3, c_inits, c_obs_ij, c_ends, c_singles = make_counts(train20)\n",
    "print \"parameters for dataset1(train20)\"\n",
    "print \"Number of states(correct typed letters)  : \"+str(len(c_states))\n",
    "print \"Nomber of observations(actually typed letters)  : \"+str(len(c_observations))\n",
    "print \"Number of pairs(state-obervations): \"+str(len(c_pairs))\n",
    "print \"Number of first order trans(state(t-1)-state(t)) : \"+str(len(c_transitions_2))+ \" / \"+ str(26*26)\n",
    "print \"Number of second order trans(state(t-1)-state(t)) : \" + str(len(c_transitions_3))+ \" / \"+ str(26*26*26)\n",
    "print \"Nombre de init. : \"+str(len(c_inits))\n",
    "print \"Number of obs_ij:\" +str(len(c_obs_ij))\n",
    "print \"Number of single letter word:\" +str(len(c_singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.supervised_training(c_inits, c_pairs, c_transitions_2, c_singles, c_states, c_transitions_3, c_obs_ij, c_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hmm.predict(test20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors exists:3239.0\n",
      "Number of errors the model correct1966.0\n",
      "Number of errors the model create848.0\n",
      "Accurency:0.895452639147\n",
      "Precision:0.698649609097\n",
      "Recall:0.686692280824\n"
     ]
    }
   ],
   "source": [
    "hmm.score(test20, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
